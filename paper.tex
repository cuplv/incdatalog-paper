%% For double-blind review submission
\documentclass[acmlarge,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission
%\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format should change 'acmlarge' to
%% 'sigplan,10pt'.


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

\usepackage{listings}
\lstset{}

\usepackage{hyperref}
\hypersetup{colorlinks = true,linkcolor = blue, urlcolor=blue}

\usepackage{tikz}
\usetikzlibrary{tikzmark}

\usepackage{xcolor}


\makeatletter\if@ACM@journal\makeatother
%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2017}
\acmMonth{1}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother
%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
\acmYear{2017}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
\setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations

% Darker versions (dRed, dBlue, etc.) of common colours;
% pure red/blue/green are violently garish, and too light in monochrome.
%
% dDkRed, etc. are darker still.
% dLight[er]{Blue|Purple} are lighter colours suitable for backgrounds.
%
% dFaint, dGray, dDark, dAlmostBlack are shades of gray.
%
\definecolor{dRed}{rgb}{0.65, 0.0, 0.0}
\definecolor{dDkRed}{rgb}{0.35, 0.0, 0.0}
\definecolor{dGreen}{rgb}{0.0, 0.65, 0.0}
\definecolor{dDkGreen}{rgb}{0.0, 0.35, 0.0}
\definecolor{dBlue}{rgb}{0.0, 0.0, 0.65}
\definecolor{dLightBlue}{rgb}{0.4, 0.4, 0.9}
\definecolor{dLighterBlue}{rgb}{0.8, 0.8, 1.0}
\definecolor{dDkBlue}{rgb}{0.0, 0.0, 0.45}
\definecolor{dLightPurple}{rgb}{0.9, 0.5, 0.9}
\definecolor{dLighterPurple}{rgb}{1.0, 0.7, 1.0}
\definecolor{dPurple}{rgb}{0.65, 0.0, 0.65}
\definecolor{dDigPurple}{rgb}{0.5, 0.0, 0.5}
% \definecolor{DDIGPURPLE}{rgb}{0.5, 0.0, 0.5}  % laughable
\definecolor{dFaint}{rgb}{0.7, 0.7, 0.7}
\definecolor{dGray}{rgb}{0.5, 0.5, 0.5}
\definecolor{dDark}{rgb}{0.2, 0.2, 0.2}
\definecolor{dAlmostBlack}{rgb}{0.1, 0.1, 0.1}

%%% ------- Fungi code in latex listings:
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.6,0,0}
\definecolor{darkpurple}{rgb}{0.5,0,0.5}
\lstset{emph={Nm, Nat, Ref, Type, type, match, with, if, then, else, not, in, let, letrec, ref, get, thunk, force, scope, leaf, NmSet, Nat, Nm, idxtm, nmtm},
  %emphstyle={\color{blue}\bfseries},
  emphstyle={\bfseries},
  keywordstyle={\bfseries},
  commentstyle={\color{darkpurple}\ttfamily},
  language=C,
  showstringspaces=false,
}%
\lstset{emph={[2]%  
    true, false,
    },emphstyle={[2]\color{darkgreen}\bfseries}%,
}%
\newcommand{\code}[1]{\lstinline[basicstyle=\ttfamily]|#1|}
\newcommand{\hlcode}[1]{{\hl{\texttt{#1}}}}
\newcommand{\vttfamily}{\fontfamily{cmvtt}\selectfont}
\newcommand{\remark}[1]{\textcolor{darkred}{$\blacktriangleright$#1$\blacktriangleleft$}}

\lstset{
%basicstyle=\fontsize{7}{8}\ttfamily,
basicstyle=\fontsize{8}{9}\ttfamily,
mathescape=true,
literate={lam~}{$\lambda$}1
         %%%%%%%%%%% Arrows
         %%%%%%%%%%% ---------------------
         {...}{${\color{blue}{\cdots}}$}3
         {=>}{$\Rightarrow$}2
         {<=}{$\leq$}2
         {->}{$\rightarrow$}2
         {nm->}{$\rightarrow$}2 %-- Name term function sort
         {idx->}{$\rightarrow$}2 %-- Index term function sort
         {idx=>}{$\Rightarrow$}2 %-- Kind arrow: sort to kind
         %% {->}{${\color{dBlue}{\rightarrow}}$}2
         %% {nm->}{${\color{dPurple}{\rightarrow}}$}2 %-- Name term function sort
         %% {idx->}{${\color{dRed}{\rightarrow}}$}2 %-- Index term function sort
         %% {idx=>}{${\color{dRed}{\Rightarrow}}$}2 %-- Kind arrow: sort to kind
         %%%%%%%%%% Balanced delims:
         %%%%%%%%%% --------------------------------
         % {{ ... }} = Name set literals (index terms)
         {<~}{${\color{dRed}{\left\{\right.}}$}1
         {~>}{${\color{dRed}{\left.\right\}}}$}1
         % [[ ... ]] = Annotations of types with index terms (e.g., name sets)
         {[[}{${\color{dRed}{\left[\right.}}$}1
         {]]}{${\color{dRed}{\left.\right]}}$}1
         % <[ ... ]> = Annotations of names (e.g., Write scope)
         {<[}{${\color{dRed}{\left<\right.}}$}1
         {]>}{${\color{dRed}{\left.\right>}}$}1
         % [< ... >] = Thunk boundaries in concrete expression syntax
         {[<}{${\color{blue}{\left\{\right.}}$}1
         {>]}{${\color{blue}{\left.\right\}}}$}1
         % << ... ,, ... >> = Binary name composition; primitive form
         {<<}{${\color{dPurple}{\left<\right.\hspace{-4pt}\left<\right.}}$}1
         {>>}{${\color{dPurple}{\left.\right>\hspace{-4pt}\left.\right>}}$}1
         {,,}{${\color{dPurple}{,}}$}1
         %%%%%%%%%%
         %%%%%%%%%% Operators, and index term syntax:
         %%%%%%%%%% ---------------------------------
         {~@~}{${\color{dPurple}{\bullet}}$}1 %-- Binary name composition (for Nm sort)
         {~@@~}{${\color{dRed}{\bullet}}$}1 %-- Binary name composition (for NmSet sort)
         {~*}{${}^{{\color{dRed}{\ast}}}$}1 %-- Kleene star; for an index term function over an initial set
         {@}{{\textcolor{violet}{@}}}1 %-- Name literal/constant
         {^}{${\color{dRed}{\bot}}$}1 %--- Apart symbol
         {forall~}{${\color{dRed}{\forall}}$}1 %--- Forall index terms
         {exists~}{${\color{dRed}{\exists}}$}1 %--- Exists index term
         {writeset~}{${\color{dRed}{\rhd}}$}1  %--- Write set (part of a computation type)
         %%%%%%%%% Sorts:
         %{NmSet}{{\color{dRed}{NmSet}\color{black}}}4
         %% {leaf}{{\color{dPurple}{leaf}\color{black}}}3
         %% {Nm~}{{\color{dPurple}{Nm}\color{black}}}2
         %% %{Nat~}{{\color{dRed}{Nat}\color{black}}}4
         {Nat~}{Nat}4
         %% %%%%%%%%%% Declarations
         %% %%%%%%%%%% ------------------------------------
         %% {idxtm}{{\color{dRed}{idxtm}\color{black}}}4
         %% {nmtm}{{\color{dPurple}{nmtm}\color{black}}}4
}


\begin{document}

%% Title information
\title[Short Title]{Patchwork: Incremental Computation for Abstract Interpretation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
\subtitle{Subtitle}                     %% \subtitle is optional
\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Paper note
%% The \thanks command may be used to create a "paper note" ---
%% similar to a title note or an author note, but not explicitly
%% associated with a particular element.  It will appear immediately
%% above the permission/copyright statement.
\thanks{with paper note}                %% \thanks is optional
                                        %% can be repeated if necesary
                                        %% contents suppressed with 'anonymous'


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  Proposed abstract jedi text:
  \begin{itemize}
  \item Program analysis, to be useful for developers, must surface results quickly during development loop.
  \item Recomputing analysis with each edit is inefficient, performs redundant work
  \item Existing incremental techniques express limited class of analyses
  \item Our technique takes an interpretative approach, avoiding the need to compile a program to a set of facts
  \item This allows us to express a broader set of analyses over user-defined languages
  \item Key insight: fine grained memo-matching on analysis execution traces, adapton-style dependency graph.
  \end{itemize}
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}  %% \keywords is optional


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

Programmers who use static analyses in their normal workflow will frequently demand the results of these analyses multiple times.
For example, when working on code in an IDE, a programmer may make multiple small edits and request the results of an analysis after each one.
However, since edits typically affect only a small region of the program, it is very inefficient to recompute a whole-program analysis with each edit.

This is a prime application for \textit{incremental computation} (insert typical IC story). 

Existing approaches are in what we call 'Datalog-style', wherein a program is transformed into a set of initial facts, and the analysis consists of applying a series of rules until a fixed-point is reached. Examples of this approach include Flix (non-incremental) and IncA (incremental). This approach loses access to the underlying program as the analysis runs, which poses problems for incremental computation. In particular, IncA cannot express analyses requiring the use of non-monotonic widening operators, which includes most that use infinite-height lattices, such as interval or \remark{compelling examples here}.

We present the Patchwork framework, which uses Adapton's DCG-based incremental computation. This approach enables us to bring incremental programming's efficiency to general static analysis over arbitrary user-defined languages. Once the user writes a transfer function over their desired language for their desired analysis, Patchwork executes the analysis and performs any desired re-computations quickly.

We make the following contributions:

\begin{enumerate}
    \item We propose an efficient incremental naming strategy for the worklist algorithm over a CFG.
    \item We provide an incremental abstract interpretation framework which can express any domain of interest. (Wording? Any domain + widening operator? Point is frees the monotonicity restriction)
    \item We demonstrate that our framework is competitive, performance-wise, with 'Datalog-style' alternatives.
\end{enumerate}

\begin{figure}
\begin{lstlisting}
type Q
type Inv
type VisitRes = NoChange | Change Inv

let visitloc : Inv -> Loc -> VisitRes =
  lam~inv.lam~loc.
    ...
    ...   // subcase: Update the invariant mapping:
    ...   let inv' = update inv loc absstate ;
    ...   Change inv'
    ...

letrec loop : Inv -> Q -> Inv =
  lam~inv.lam~q.
    match pop q with
      None => inv
      Some q loc =>
         match visitloc loc inv with
           NoChange   => loop inv q
           Change inv => loop inv (push q (succof loc))    
\end{lstlisting}
\caption{Analysis loop: Each ``visit'' to a program location has the
  potential to \code{update} the invariant mapping \code{inv}; the
  queue~{q} controls the schedule of these visits. }
\end{figure}

\begin{figure}
\begin{lstlisting}
/// Version with explicit naming strategy
type Q[X:NmSet]
type Inv[X:NmSet]
type VisitRes[X:NmSet] = NoChange | Change (Inv[X])

let visitloc : Inv[X] -> Loc[Y] -> exists Z|Z^X. VisitRes[X^Z] =
  lam~inv.lam~loc.
    ...
    ...   // Construct a name Z such that Z^X
    ...   let name : Nm[Z] = ... ;
    ...   let inv' = update inv name loc absstate ;
    ...   Change inv'
    ...

letrec loop : Inv[X] -> Q[Y] -> exists Z|Z^X. Inv[X^Z] =
  lam~inv.lam~q.
    match pop q with
      None => inv
      Some q loc =>
         match visitloc loc inv with
           NoChange   => loop inv q
           Change inv => loop inv (push q (succof loc))
\end{lstlisting}
\caption{Analysis loop: Each ``visit'' to a program location has the
  potential to \code{update} the invariant mapping \code{inv}; the
  queue~{q} controls the schedule of these visits. }
\end{figure}

%\newcommand{\TabYes}{\ensuremath{\ding{52}}}
%\newcommand{\TabNo}{\ensuremath{\ding{53}}}
\newcommand{\TabYes}{\ensuremath{\blacksquare}}
\newcommand{\TabNo}{\ensuremath{\square}}

\begin{table}
\begin{tabular}{|p{1.3in}p{1.5in}||ccc|l|}
  \hline
  \textbf{program domain} &
  \textbf{system feature} & Flix & IncA & Datafun & \emph{this paper}
  \\
  \hline
  general &
  first-class  
  fixed points & \TabNo & \TabNo & \TabYes & \TabYes
  \\
  general &
  user-defined
  datatypes & \TabYes & ? & \TabYes & \TabYes
  \\
  \hline
  datalog &
  datalog engine & \TabYes & \TabYes & \TabYes & \TabYes~(definable)
  \\
  \hline
  program analysis &
  user-defined lattices & \TabYes & \TabYes & \TabNo & \TabYes
  \\
  program analysis &
  non-monotone 
  widening
  & \TabYes & \TabNo & n/a & \TabYes
  \\
  program analysis &
  relational 
  analysis
  & ? & \TabNo? & n/a & \TabYes
  \\
  \hline
\end{tabular}
\caption{By-feature comparison with related work on datalog semantics, and (incremental) datalog for program analysis.}
\label{tab:relatedwork}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|c|l|c|c|c|c|}
        \hline
        \# & stmt & result & 1: x = 9 & 6: y += 2 & 4: x -= 2 \\
        \hline
        1 & x = 10; & & & &\\
        \hline
        2 & y = 0; & x + y $\le$ 10 & \textcolor{red}{x + y $\le$ 9} & x + y $\le$ 9 & x + y $\le$ 9\\
        \hline
        3 & while x > 0: & x + y $\le$ 10 & \textcolor{red}{x + y $\le$ 9} & \textcolor{red}{x + y $\le \infty$} & \textcolor{red}{x + y $\le$ 9}\\
        \hline
        4 & \hspace*{0.3cm} x- - & x + y $\le$ 9 & \textcolor{red}{x + y $\le$ 8} & \textcolor{red}{x + y $\le \infty$} & \textcolor{red}{x + y $\le$ 7}\\
        \hline
        5 & \hspace*{0.3cm} if* & & & &\\
        \hline
        6 & \hspace*{0.6cm} y++ & x + y $\le$ 10 & \textcolor{red}{x + y $\le$ 9} & \textcolor{red}{x + y $\le \infty$} & \textcolor{red}{x + y $\le$ 9}\\
        \hline
        7 & assert(x + y $\le$ 10) & true & true & \textcolor{red}{false} & \textcolor{red}{true}\\
        \hline
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table}

\section{Motivating Examples}

Parity analysis (C1: demonstrates naming strategy)\\
Interval analysis (C2: demonstrates extra expressivity)\\
(C3 handled empirically in evaluation section)

\section{Contribution: Naming Strategy for Worklist Algorithms}

We present our naming strategy for the generic worklist algorithm. You are convinced that this naming strategy gives some efficiency guarantee for an arbitrary analysis using this algorithm.\\

We contextualize our strategy via our simplest motivating example (parity).

\section{Contribution: Expressive Abstract Interpretation Framework}

We use the discussed algorithm in a modular framework for implementing abstract interpretation over arbitrary domains. Our framework requires the implementation of an abstract domain, including transfer function, to be given by the user. This approach allows us to take advantage of existing libraries used in the static analysis community. For this paper, we interfaced with APRON, an existing library supporting the numerical domains necessary for interval and octagon analysis. Once we have defined the language to operate on - in our case, Jimple - and the transfer function for our domain of choice (see examples), we can incrementally execute the analysis.

\section{Worked examples}

go back through motivating examples: parity analysis

\begin{lstlisting}
1 x = 1;
2 while x < rand()
3   x++;
\end{lstlisting}


\subsection{Proposed Example with Relational Domain}

Consider the following simple program, which takes 1000 random walks of length 3 in $\mathbb{Z}$ and tracks how many walks end at each possible integer. (example taken from Min\'e \href{https://pdfs.semanticscholar.org/ccb9/2bfe24199455d7c4a430f756c915cd4e5ae8.pdf}{slides})
\begin{lstlisting}
1 hit = {-3:0, -1:0, 1:0, 3:0}
2 for k = 1 to 1000:
3   x = 0
4   for i = 1 to 3:
5     if random() then x++ else x--
6   hit[x] = hit[x] + 1
\end{lstlisting}

A non-relational analysis would be unable to show that \texttt{x} is in the domain of \texttt{hit} on line 6, since such a proof depends on the relationship between $i$ and $x$ in the inner for loop.  However,  using a relational abstract domain such as octagon, we can express the constraint that $|x|\leq i$ in the inner for loop.  Note that this is still insufficient to show that the result will always be odd; we could, however, combine with a parity domain to show that, not only are the results in $[-3,3]$ (from octagon), they're also in $2\mathbb{Z}+1$ (from parity).

We can easily vary the number of random walks taken or the length of each walk to ``grow'' the program and show asymptotic scaling.  However, if I understood Matt's messages correctly, that doesn't address the question of an ``edit model''.  I'll be talking to Jared about what such an edit model might look like here.

\iffalse NOTE(benno): also considered a couple other examples, noting here as alternatives that could be further pursued if neeeded:

This one needs relational analysis to statically discharge the assertion, but there's no good way I see initially to vary the size of the program.
  x = 0;
  y = 0;
  if rand():
    x = 1
  else:
    y = 1
    assert !(x && y)

This one comes directly from the original octagon analysis paper.  It also has a nice relational story, but is similarly hard to vary the size of the program, though it is straightforward to change some initial parameters for incremental edits.
    x = 10
    y = 0
    while(x > 0):
      x--;
      if(*):
        y++
    assert(x+y <= 10)  
A non-relational analysis would not be able to statically discharge the assertion, concluding only that $x\in (-\infty,10]$ and $y\in [0,\infty)$.  However, a relational domain like octagon is able to statically discharge the assertion using a loop invariant of $x+y\leq10\wedge-x-y\leq10$

I also played around for a while with string substring operations (where we'd have to relate the length of the substring to the total length of the list and the beginning index to prove accesses safe) but couldn't come up with a clean simple example to work with.



\fi

\begin{table}[]
  \centering
  \newcommand{\location}{\tikz\draw[green!65!black,fill=green!65!black] (0,0) circle (.4ex);}
  \def\arraystretch{2}
    \begin{tabular}{r|l|ccl}
        \# & Program Location & \multicolumn{3}{l}{Analysis State Progression} \\
        \midrule
        1 & \texttt{x = 10;\location} &\quad&$\{\}$\tikzmark{1a} & \\
        \hline
        2 & \texttt{y = 0;\location} &$\bot$\quad& $\{x+y\leq10\}$\tikzmark{2a} & \hspace{1em}$\{x+y\leq10\}$\tikzmark{2b}\\
        \hline
        3 & \texttt{while(x > 0)\{\location} &$\bot$\quad& $\{x+y\leq10\}$\tikzmark{3a} & \tikzmark{3b}\\
        \hline
        4 & \quad\texttt{x--;\location} &$\bot$\quad&  $\{x+y\leq9\}$\tikzmark{4a}& \\
        \hline
        5 & \quad\texttt{if(*) \{y++;\location\}} &$\bot$\quad&$\{x+y\leq10\}$\tikzmark{5a}& \\
        \hline
        6 & \quad\texttt{else \{$\epsilon$;\location\}} &$\bot$\quad& $\{x+y\leq9\}$\tikzmark{6a} & \\
        \hline
        7 & \location\texttt{\}} &$\bot$\quad& $\{x+y\leq10\}$\tikzmark{7a} & \tikzmark{7b}\\
        \hline
        8 & \location~\texttt{assert(x + y $\le$ 10)} &$\bot$\quad& $\{x+y\leq10\}$\tikzmark{8a} & \\
    \end{tabular}

    \newcommand{\abovetext}{10pt}
    \newcommand{\belowtext}{-4pt}
    
    \begin{tikzpicture}[overlay,remember picture, shorten >=-3pt]
      \coordinate (1a) at ([xshift=-0.46em]pic cs:1a) ;
      \coordinate (2a) at (pic cs:2a);
      \coordinate (3a) at (pic cs:3a);
      \coordinate (4a) at (pic cs:4a);
      \coordinate (5a) at (pic cs:5a);
      \coordinate (6a) at (pic cs:6a);
      \coordinate (7a) at (pic cs:7a);
      \coordinate (8a) at (pic cs:8a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a) -- ([yshift = \abovetext]1a|-2a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-2a) -- ([yshift = \abovetext]1a|-3a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-3a) -- ([yshift = \abovetext]1a|-4a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-4a) -- ([yshift = \abovetext]1a|-5a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-4a)
      to [out=270,in=90] ([xshift=0.6em,yshift=0.2em] pic cs:5a)
      to [out=270,in=90] ([yshift = \abovetext]1a|-6a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-5a)
      to [out=270,in=90] ([xshift=-5.25em,yshift=0.25em] pic cs:6a)
      to [out=270,in=90] ([yshift = \abovetext]1a|-7a);
      \draw[->, blue!75!black] ([yshift=\belowtext]1a|-6a) -- ([yshift = \abovetext]1a|-7a);
      \draw[->, blue!75!black,rounded corners=4pt] ([yshift=4pt,xshift=1pt] pic cs:7a) -| ([xshift=-1pt] pic cs:3b) |- ([xshift=-5.5em, yshift=3pt]pic cs:2b);
%      \draw[->, blue!75!black,rounded corners=4pt] ([yshift=4pt,xshift=1pt] pic cs:7a) -| ([yshift=-5pt, xshift=-1pt] pic cs:7b) |- ([xshift=-5.5em, yshift=3pt]pic cs:8b);
      \draw[->, blue!75!black,rounded corners=3pt] ([yshift=3pt,xshift=-5.25em] pic cs:2a) -| ([xshift=-5.65em,yshift=0.35em] pic cs:6a) |- ([yshift=3pt,xshift=-5.5em] pic cs:8a);
    \end{tikzpicture}
    
    \caption{ Visualization of fix-point computation for a program analyzer performing octagon analysis on the example program from Fig.~\ref{???}.
      Program locations in the abstract syntax are denoted by green circles in the concrete syntax.  The blue arrows illustrate a ``trace'' of the worklist/fix-point computation, showing the propagation of dataflow analysis results through the program.
%      Thus, the full history of partially-computed abstract states at a particular location is recorded from left to right in the ``Analysis State Progression'' column.
    }
    \label{tab:ATT}
\end{table}

\section{Evaluation}

Our evaluation seeks to answer several core research questions:

\begin{enumerate}
    \item Does Patchwork achieve significant performance gains on incremental edits relative to a non-incremental baseline tool? (check overhead/speedup constants)
    \item Is Patchwork competitive with alternative tools for incremental static analysis (IncA)? (direct comparison on parity analysis)
    \item Is the added expressivity of Patchwork relevant to clients? (check false alarm rates on impoverished interval analysis v. regular)
\end{enumerate}

We evaluated Patchwork on a series of benchmark programs (IncA's) using parity analysis, bad interval analysis, and normal interval analysis. We compare Patchwork's speed to Jandom on parity analysis (just that?). We compare Patchwork's speed to IncA on bad interval analysis. We compare the false alarm rate of the normal interval analysis to that of the bad interval analysis.

\section{Future work/Extensions}

Interprocedural? Or doing that now?\\
Improving naming strategy? Or specializing it to different domains?\\
Incremental relation data structure?

\section{Related Work}

Incremental Datalog framework (which?):

Flix: Flix enriches the basic semantics of Datalog with lattices, making static analysis significantly easier to write. However, Flix is still tied to the core Datalog model - compiling a program to an initial set of facts, then applying a set of productions rules to a fixpoint - which limits the efficiency of the algorithm. Specifically, Flix evaluates (a lot) of rules in the course of (example), including a large number of redundant evaluations, due to (facts about their dependency analysis). The DCG-based approach automatically tracks dependencies in a much better (why, specifically) model, which improves efficiency.

IncA: IncA effectively builds on work like that in Flix by adding explicit support for incremental computation. After running a static analysis, the user can modify the initial set of facts (implicitly modifying the program) and re-execute with some improvements in performance. IncA's approach takes advantage of properties of monotonic operators on lattices to quickly update analysis results for certain types of edits. However, the restrictions that all operators must be monotonic rules out common and important program analysis, such as (examples from above). For instance, IncA's framework could not express (our worked example) - they would instead have to use (different widening operator), which would significantly degrade the strength of the analysis (for reasons).

\section{Conclusion}

Existing state-of-the-art approaches to incremental program analysis are not fully general. Incremental Datalog engines are restricted to the simple declarative model, and even more complex approaches such as IncA restrict the types of operators an analysis user can work with. The Patchwork framework provides a fully general (is this true?) approach to incremental static analysis. Our use of Adapton also improves efficiency by (numbers). This represents a significant advance in the state of incremental static analysis.



%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
%\bibliography{bibfile}


%% Appendix
\appendix
\section{Appendix: Datalog Strategy - Limitations + Comparison}

The Datalog-style approach to static analysis uses the following broad algorithm:

\begin{enumerate}
    \item Define a set of relations
    \item Define a set of rules that refer to these relations
    \item Transform the input program into a set of 'facts', which are judgements of membership in the relations
    \item Apply the rules to generate new facts until a fixpoint is reached
\end{enumerate}

\subsection{Example}

As an example, consider the simple points-to analysis defined by the Flix authors (From Datalog to FLIX: A Declarative Language for Fixed Points on Lattices, section 2). The approach is applied as follows:

\begin{enumerate}
    \item Define a set of relations.\\
    For this problem, there are 6 relations total - 4 "input relations", which are the base facts, and 2 "derived relations", which give the information we're interested in. They are as follows: New(v, h), Assign(v, v), Store(v, f, v), Load(v, v, f) are the inputs, and VarPointsTo(v, h) and HeapPointsTo(h, f, h) are the derived relations. Here the lowercase letters are types - v for variable, h for heap, f for field.
    
    \item Define a set of rules that refer to these relations.\\
    There are four rules that define this analysis:
    \begin{enumerate}
        \item VarPointsTo(v1, h1) :- New(v1, h1)
        \item VarPointsTo(v1, h2) :- Assign(v1, v2), VarPointsTo(v2, h2)
        \item VarPointsTo(v1, h2) :- Load(v1, v2, f), VarPointsTo(v2, h1), HeapPointsTo(h1, f, h2)
        \item HeapPointsTo(h1, f, h2) :- Store(v1, f, v2), VarPointsTo(v1, h1), VarPointsTo(v2, h2)
    \end{enumerate}
    For each of the rules, the fact on the left is added to the database if all of the facts on the right are in the database.\\
    
    \item Transform the input program into a set of 'facts'.
    \begin{lstlisting}[language=Java]
    ClassA o1 = new ClassA(); // object A
    ClassB o2 = new ClassB(); // object B
    ClassB o3 = o2;
    o2.f = o1;
    Object r = o3.f;
    \end{lstlisting}
    
    This program is transformed into the following facts:\\
    New("o1", "A")\\
    New("o2", "B")\\
    Assign("o3", "o2")\\
    Store("o2", "f", "o1")\\
    Load("r", "o3", "f")\\
    
    The algorithm for this transformation is not given. For this simple example, it's fairly easy to see how each fact was derived from the program. Some algorithm for this transformation must be written for any analysis in this style.\\
    
    \item Apply the rules to generate new facts until a fixpoint is reached.\\
    This results in the following derived relations:\\
    VarPointsTo("o1", "A") - rule 1\\
    VarPointsTo("o2", "B") - rule 1\\
    VarPointsTo("o3", "B") - rule 2\\
    HeapPointsTo("B", "f", "A") - rule 4\\
    VarPointsTo("r", "A") - rule 3\\
    
    In the example, the final fact is what was desired.
\end{enumerate}

\subsection{Naive Incremental Computation in the Datalog World}

In the Datalog context, an edit consists of the insertion or deletion of facts to the starting database. Wlog, we consider insertions and deletions of single facts and discuss naive incremental strategies for these edits.

In the course of the Datalog engine's run, new facts are added to the database when a rule is triggered. It is easy to construct a simple 'provenance' of each fact in the database by tagging the fact with the facts that generated it. For instance, in the example from 3.1, the VarPointsTo("o3", "B") was generated via the facts Assign("o3", "o2") and VarPointsTo("o2, "B"), the latter of which was itself generated from New("o2", "B"). Assuming that in the course of the algorithm these provenances are stored, consider now an edit which consists of the deletion of the fact New("o2", "B") from the initial database. Rather than re-computing from scratch, we can start from our finished database and do the following: delete the fact New("o2", "B"), delete all facts that transitively depend on New("o2", "B"), use the remaining facts as the initial database for a new run. Crucially, rather than re-computing all the facts from scratch, using this algorithm, any computed fact which does not depend on the deleted fact does not have to be re-computed.

Now consider an edit which consists of the insertion of an arbitrary fact. This does not affect the facts in the database, which means we can do the following: simply use the finished database + the new fact as the initial database for a new run. But, what if the fact adds to a relation that is negated in one of our rules? Then the insertion of this fact is also a deletion of the corresponding negation, and the deletion should be processed first. (Datalog engines typically have restrictions on the use of negation in their rules - not discussed here in detail)

Therefore, a naive incremental algorithm for an arbitrary set of deletions and insertions $\Delta\{d_k, i_k\}$ is as follows: we have a final database $D$. For each deletion $d_k$ , delete $d_k$ and all facts that transitively depend on it from $D$, leaving us with $D'$. Then add each inserted fact to get $D' + \{i_k\}$, and do a new run starting with this database.

\subsection{IncA's Improved Approach}

The above are the simplest two incremental strategies which many Datalog engines (even those that do not advertise themselves as incremental) implement. IncA, an explicitly incremental framework for Datalog (+ lattices), adds an additional optimization in the context of rules over lattices. Assuming all functions used are \textit{monotone}, a deletion of fact $d$ and insertion of fact $i$ where $d \sqsubseteq i$ constitutes a \textit{monotonically increasing replacement} (term introduced by IncA authors). If all functions used are monotone, then it is true that any fact derived from $d$ is also derivable from $i$ (proof left to IncA folks). Therefore, rather than deleting many facts derived from $d$ and subsequently re-deriving all of them, we don't have to perform any deletions.

Therefore IncA's improved incremental algorithm is as follows. Given a final database $D$ and an arbitrary set of deletions and insertions $\Delta\{d_k, i_k\}$, first identify which $d_c$ have corresponding $i_c$ such that $d_c \sqsubseteq i_c$. Delete each of these from $\{d_k\}$ to get the strictly smaller set $\{d'_k\}$. Then run the above naive algorithm on $\Delta\{d'_k, i_k\}$

There are some additional implementation details to IncA's algorithm that let it efficiently perform these deletions/re-derivations, but those do not change the improvement that's being made in terms of incremental computation, so we elide those details here.

\subsection{Limitations}

Once the Datalog strategy is chosen, there are several inherent limitations with respect to incremental computation. We briefly outline several of them here, and gesture toward how these are overcome using the Adapton strategy.\\

Choosing the Datalog strategy does not allow the user to directly reference the program of interest in their analysis. Instead, they must 'compile' the program into a new set of facts/relations relevant to their analysis. Often, as in IncA's interval analysis, a considerable amount of scaffolding relations are necessary to retain awareness of the program's control flow, and the user must correctly connect the program's semantics to the relations and types written for Datalog, which requires consideration of two distinct domains. In contrast, the Adapton approach allows us to directly write abstract transfer functions over the concrete semantics of our language of choice, removing a layer of abstraction.\\

Both Flix and IncA require that all functions over lattices are \textit{monotone}. This is a non-trivial restriction in the context of static analysis, as many standard analyses use non-monotonic widening functions to ensure termination. All of these are inadmissible for Flix or IncA. In the presence of non-monotone functions, IncA could not guarantee that monotonically increasing replacements would not require deletions. Therefore, to achieve from-scratch consistency, they would have to fallback to the naive incremental strategy. Adapton has no monotonicity restriction.\\

There is an important sense in which the approach taken by IncA lacks 'history' in the sense of Adapton. We illustrate via an example. Consider an arbitrary lattice with elements $a, b, c, d$ such that the following holds: $a$ and $b$ are not ordered, $a \sqcup c = d$, $b \sqcup c = d$, $c \neq d$. Assuming that $a$ and $c$ are in the initial set of facts, an analysis run will add $d$ to the database. Now assume we make the following edit: delete $a$ and insert $b$. This is not a monotonically increasing replacement, so IncA will transitively delete all facts depending on $a$, including $d$ and any facts derived from $d$. Now after adding $b$, in the incremental run, the analysis will quickly re-discover $d$. However, since $d$ was already deleted from the database, all consequents of $d$ must be completely re-computed. Because IncA cannot 'look ahead' to see that $d$ will be re-discovered from an inserted fact, work that could have been saved is lost. This is a specific consequence of the general problem that deleting all facts deriving from a deleted fact is an over-approximation that is necessary for the Datalog approach to be sound. In contrast, Adapton memoizes the calls which use $d$ in further derivations. When $a$ is deleted $d$ is stale, but as soon as $d$ is re-discovered, the any downstream computations memo-match and do not need to be re-computed.\\

 There have been some attempts to alleviate the over-deletion problem (B/F alg \url{https://www.cs.ox.ac.uk/boris.motik/pubs/mnph15incremental-BF.pdf}) (hybrid alg \url{https://arxiv.org/pdf/1711.03987.pdf}). This appears to be an improvement to Delete/Rederive algorithms, but not in all cases, and Adapton should still give you this behavior effectively for free.\\

\subsection{Advantages}

Datalog is implementation-agnostic with respect to the evaluation strategy for the engine itself. (Is this still true for IncA? I think so...)

IncA likely outperforms a more generic strategy on edits that are friendly to it (monotonically increasing replacements).

\end{document}
